# Project Progress and Tasks é¡¹ç›®è¿›å±•å’Œä»»åŠ¡

- **Bro in CUDA**: [https://github.com/a-hamdi/cuda](https://github.com/a-hamdi/cuda)
- **CUDA ä¸­çš„å…„å¼Ÿ**: [https://github.com/a-hamdi/cuda](https://github.com/a-hamdi/cuda)
- **Mentor** ğŸš€: [https://github.com/hkproj](https://github.com/hkproj) | [https://github.com/hkproj/100-days-of-gpu](https://github.com/hkproj/100-days-of-gpu)
- **å¯¼å¸ˆ**: [https://github.com/hkproj](https://github.com/hkproj) | [https://github.com/hkproj/100-days-of-gpu](https://github.com/hkproj/100-days-of-gpu)

## Mandatory and Optional Tasks å¿…åšä»»åŠ¡å’Œé€‰åšä»»åŠ¡

| Day å¤© | Task Description ä»»åŠ¡æè¿° |
|--------|----------------------------|
| D15    | **Mandatory FA2-Forward**: Implement forward pass for FA2 (e.g., a custom neural network layer). å¼ºåˆ¶ FA2-Forward: å®ç° FA2 çš„å‰å‘ä¼ é€’ï¼ˆä¾‹å¦‚ï¼Œè‡ªå®šä¹‰ç¥ç»ç½‘ç»œå±‚ï¼‰ã€‚ |
| D20    | **Mandatory FA2-Backwards**: Implement backward pass for FA2 (e.g., gradient computation). å¼ºåˆ¶ FA2-åå‘ä¼ é€’: å®ç° FA2 çš„åå‘ä¼ é€’ï¼ˆä¾‹å¦‚ï¼Œæ¢¯åº¦è®¡ç®—ï¼‰ã€‚ |
| D20    | **Optional Fused Chunked CE Loss + Backwards**: Fused implementation of chunked cross-entropy loss with backward pass. Can use Liger Kernel as a reference implementation. å¯é€‰çš„èåˆåˆ†å—äº¤å‰ç†µæŸå¤± + åå‘ä¼ é€’: èåˆåˆ†å—äº¤å‰ç†µæŸå¤±çš„åå‘ä¼ é€’å®ç°ã€‚å¯ä»¥å‚è€ƒ Liger Kernel ä½œä¸ºå‚è€ƒå®ç°ã€‚ |

# Project Progress by Day

| Day   | Files & Summaries |
|-------|-------------------|
| day1  | **RMSnorm_vec.cu**: Implementation of RMS normalization using CUDA, including warp and block-level reduction for efficient computation. |
| day2  | **FP16_GELU.cu**: Implemented a vectorized FP16 GELU kernel and benchmarked it against PyTorch's implementation. |
| day3  | **gemv.cu**: Implemented a vectorized FP32 GEMV kernel and benchmarked it against PyTorch's implementation. **Issue**: Bandwidth measurement problem remains unresolved. |
| day4  | **per_tensor_quantize.cu**: Implemented per-tensor symmetric and asymmetric INT8 quantization. Results were verified on a CPU. |
| day5  | **fused_bias_mask_scale_and_add_fp16.cu**: Implemented a vectorized FP16 fused bias, mask, scale, and add kernel. |
| day6  | **optimized_kernel.cu**: |
|       | 1. v1: Added global memory coalesced access and shared memory caching |
|       | 2. v2: Implemented shared memory and sliding window technique |
|       | 3. v3: Developed strided shared memory access to improve compute intensity |
|       | 4. v4: Created vectorized reads based on v3 |
| day7  | **sgemm_kernel_fp32.cu**: |
|       | 1. v5: Implemented vectorized reads, but column-wise access of matrix B caused bank conflicts |
|       | 2. v6: Transformed inner product to outer product in shared memory, implemented secondary caching using registers, but without float4 reads |
|       | 3. v7: Based on v6, implemented float4 reads in N-direction of the block using register indexing |
|       | 4. v8: Added transpose storage of matrix A to shared memory via register buffering |
|       | 5. v9: Introduced double buffering technique using two shared memory buffers to hide memory loading latency |
| day8  | **attention_mask.cu**: |
|       | 1. å®ç°äº†åŸºæœ¬çš„attention mask CUDA kernelï¼Œæ”¯æŒä¸åŒåºåˆ—é•¿åº¦çš„maskè®¡ç®— |
|       | 2. è€ƒè™‘äº†repeat kvï¼ˆkey-valueé‡å¤ï¼‰çš„æƒ…å†µï¼Œä½†å°šæœªè¿›è¡Œå……åˆ†æµ‹è¯• |
| day9  | **cublas_gemm.cu**: |
|       | 1. å®ç°äº†ä½¿ç”¨cublasè¿›è¡ŒçŸ©é˜µä¹˜æ³•çš„åŸºæœ¬ç¤ºä¾‹ |
|       | 2. ç‰¹åˆ«æ³¨æ„ï¼šcublasé»˜è®¤ä½¿ç”¨åˆ—ä¸»åºå­˜å‚¨ï¼ˆcolumn-majorï¼‰ï¼Œä¸PyTorchçš„è¡Œä¸»åºï¼ˆrow-majorï¼‰ä¸åŒ |
|       | 3. è§£å†³æ–¹æ³•ï¼šåœ¨ä½¿ç”¨cublasæ—¶ï¼Œå¯ä»¥é€šè¿‡è½¬ç½®è¾“å…¥çŸ©é˜µæ¥æ¨¡æ‹Ÿè¡Œä¸»åºè¡Œä¸º |
|       | 4. æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨cublasLt APIè¿›è¡Œæ›´çµæ´»çš„çŸ©é˜µä¹˜æ³•é…ç½® |
| day10 | **softmax_fp32.cu**: |
|       | 1. å®ç°äº†ä¸€ä¸ªç®€æ´çš„FP32 softmaxç®—å­ |
|       | 2. ä½¿ç”¨äº†warpçº§åˆ«çš„å¹¶è¡Œå½’çº¦æ¥ä¼˜åŒ–æ€§èƒ½ |
|       | 3. è¿›è¡Œäº†æ•°å€¼ç¨³å®šæ€§æµ‹è¯•ï¼Œç¡®ä¿åœ¨è¾“å…¥å€¼è¾ƒå¤§æ—¶ä¸ä¼šå‡ºç°æ•°å€¼æº¢å‡ºé—®é¢˜ |
|       | 4. ä¸PyTorchçš„softmaxå®ç°è¿›è¡Œäº†å¯¹æ¯”æµ‹è¯•ï¼Œç»“æœä¸€è‡´ |
| day11 | **concat_kv.cu**: |
|       | 1. å®ç°äº†æ‹¼æ¥KVç¼“å­˜çš„CUDA kernelåŠŸèƒ½ |
|       | 2. æ”¯æŒé«˜æ•ˆåœ°å°†æ–°çš„keyå’Œvalueå‘é‡æ‹¼æ¥åˆ°ç°æœ‰çš„KVç¼“å­˜ä¸­ |
|       | 3. é’ˆå¯¹ä¸åŒåºåˆ—é•¿åº¦å’Œæ‰¹é‡å¤§å°è¿›è¡Œäº†æ€§èƒ½ä¼˜åŒ– |
|       | 4. é€šè¿‡ä¸PyTorchå®ç°çš„å¯¹æ¯”æµ‹è¯•éªŒè¯äº†æ­£ç¡®æ€§ |
| day12 | **transpose_bank_conflict.cu**: |
|       | 1. å®ç°äº†ä¸€ä¸ªç®€å•çš„çŸ©é˜µè½¬ç½®ç®—å­ï¼Œç”¨äºå­¦ä¹  shared memory ä¸­çš„ bank conflict æ¦‚å¿µ |
|       | 2. åœ¨ NVIDIA 4060 laptop ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œä½†æœªèƒ½å¤ç°åšå®¢ä¸­æåˆ°çš„ shared memory å­˜å‚¨è¿‡ç¨‹ä¸­çš„ bank conflict ä¿¡æ¯ |
|       | 3. å¯èƒ½åŸå› ï¼šç°ä»£ GPU æ¶æ„å¯¹ bank conflict è¿›è¡Œäº†ä¼˜åŒ–ï¼Œæˆ–è€…æµ‹è¯•çŸ©é˜µè§„æ¨¡ä¸å¤Ÿå¤§ |
| day13 | **hgemv.cu**: |
|       | 1. å®ç°äº†åŠç²¾åº¦ï¼ˆFP16ï¼‰çš„çŸ©é˜µå‘é‡ä¹˜æ³•ï¼ˆGEMVï¼‰ç®—å­ |
|       | 2. ä¸PyTorchçš„åŠç²¾åº¦GEMVå®ç°è¿›è¡Œäº†æ€§èƒ½å¯¹æ¯”å’Œç²¾åº¦éªŒè¯ |
| day14 | **triton_matmul.py**: |
|       | 1. å­¦ä¹ äº†ä½¿ç”¨Tritonæ¡†æ¶å®ç°é«˜æ•ˆçŸ©é˜µä¹˜æ³• |
|       | 2. å‚è€ƒTritonå®˜æ–¹æ–‡æ¡£ï¼Œå®ç°äº†åŸºäºåˆ†ç»„ï¼ˆgroupï¼‰çš„çŸ©é˜µä¹˜æ³•å†™æ³• |
|       | 3. ç†è§£äº†Tritonå¦‚ä½•é€šè¿‡åˆ†ç»„çš„å—çº§å¹¶è¡Œæ¥æå‡L2ç¼“å­˜å‘½ä¸­ä»è€Œä¼˜åŒ–äº†çŸ©é˜µè¾“å…¥è§„æ¨¡å¤§äºL2ç¼“å­˜æƒ…å†µä¸‹çš„æ•°æ®è¯»å–é€Ÿåº¦ |
| day15 | **swiglu_kernel.cu**: |
|       | 1. å­¦ä¹ äº†SwiGLUæ¿€æ´»å‡½æ•°çš„åŸç†å’Œåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ |
|       | 2. å®ç°äº†ä¸€ä¸ªç®€æ˜“çš„SwiGLU CUDA kernelï¼Œæ”¯æŒFP16å’ŒFP32æ•°æ®ç±»å‹ |
| day16 | **fused_mha.cu**: |
|       | 1. å®ç°äº†ä¸€ä¸ªèåˆå¤šå¤´æ³¨æ„åŠ›(Fused Multi-Head Attention)ç®—å­ï¼Œå°†å¤šä¸ªæ“ä½œåˆå¹¶ä¸ºä¸€ä¸ªCUDA kernel |
|       | 2. é›†æˆäº†å¤šä¸ªå…³é”®æ“ä½œï¼šKVç¼“å­˜æ‹¼æ¥(concat KV)ã€KVé‡å¤(repeat KV)ã€æŸ¥è¯¢-é”®å€¼çŸ©é˜µä¹˜æ³•(QK GEMV)ã€Softmaxå½’ä¸€åŒ–ä»¥åŠæ³¨æ„åŠ›è¾“å‡ºè®¡ç®—(QK*V GEMV) |
|       | 3. é€šè¿‡èåˆæ“ä½œå‡å°‘äº†å†…å­˜è®¿é—®å’Œkernelå¯åŠ¨å¼€é”€|
| day17 | **fp8_matmul.cu**: |
|       | 1. å‚è€ƒDeepSeek V3å¼€æºä»£ç ï¼Œå­¦ä¹ äº†Tritonæ¡†æ¶ä¸‹çš„FP8é‡åŒ–çš„çŸ©é˜µä¹˜æ³•å®ç° |
|       | 2. å®ç°äº†FP16/FP32åˆ°FP8çš„é‡åŒ–è½¬æ¢å‡½æ•°ï¼ŒåŒ…æ‹¬é‡åŒ–æ¯”ä¾‹å› å­çš„è®¡ç®— |
| day18 | **triton_flash_attention2.py**: |
|       | 1. å‚è€ƒUmar Jamilçš„è§†é¢‘æ•™ç¨‹ï¼Œå­¦ä¹ äº†Tritonç‰ˆæœ¬çš„Flash Attention 2å‰å‘ä¼ æ’­å®ç° |
|       | 2. ç†è§£äº†Flash Attention 2çš„æ ¸å¿ƒä¼˜åŒ–ï¼šé€šè¿‡åˆ†å—è®¡ç®—å’Œåœ¨çº¿softmaxæ¥å‡å°‘å†…å­˜è®¿é—® |
|       | 3. å®ç°äº†åŸºæœ¬çš„Triton kernelï¼ŒåŒ…æ‹¬åˆ†å—çŸ©é˜µä¹˜æ³•å’Œåœ¨çº¿softmaxè®¡ç®— |
| day19 | **int8_gemm.cu**: |
|       | 1. å®ç°äº†ç®€æ˜“çš„int8é‡åŒ–gemmç®—å­ï¼Œæ”¯æŒçŸ©é˜µä¹˜æ³•è®¡ç®— |
|       | 2. é‡‡ç”¨å…¨å±€é‡åŒ–ç­–ç•¥ï¼Œå°†æµ®ç‚¹æ•°çš„çŸ©é˜µä¹˜æ³•è½¬æ¢ä¸ºint8ç±»å‹çš„çŸ©é˜µä¹˜æ³• |
|       | 3. å®ç°äº†é‡åŒ–è¿‡ç¨‹ï¼šå°†FP32è¾“å…¥çŸ©é˜µé‡åŒ–ä¸ºINT8ï¼Œä½¿ç”¨INT8è¿›è¡ŒçŸ©é˜µä¹˜æ³•è®¡ç®—ï¼Œæœ€åå°†è¾“å‡ºåé‡åŒ–ä¸ºFP32 |
|       | 4. é€šè¿‡é‡åŒ–æ¯”ä¾‹å› å­ï¼ˆscale factorï¼‰æ¥ä¿æŒæ•°å€¼ç²¾åº¦ |
|       | 5. ä¸FP32çŸ©é˜µä¹˜æ³•ç»“æœè¿›è¡Œäº†å¯¹æ¯”æµ‹è¯•ï¼ŒéªŒè¯äº†é‡åŒ–è®¡ç®—çš„å‡†ç¡®æ€§ |
| day20 | **matrix_fast_pow.cpp**: |
|       | 1. å®ç°äº†C++çš„çŸ©é˜µå¿«é€Ÿå¹‚ç®—æ³•ï¼Œç”¨äºé«˜æ•ˆè®¡ç®—çŸ©é˜µçš„é«˜æ¬¡å¹‚ï¼Œè®¡ç®—å¤æ‚åº¦O(logk * n^3) |











