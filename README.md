# Project Progress and Tasks é¡¹ç›®è¿›å±•å’Œä»»åŠ¡

- **Bro in CUDA**: [https://github.com/a-hamdi/cuda](https://github.com/a-hamdi/cuda)
- **CUDA ä¸­çš„å…„å¼Ÿ**: [https://github.com/a-hamdi/cuda](https://github.com/a-hamdi/cuda)
- **Mentor** ğŸš€: [https://github.com/hkproj](https://github.com/hkproj) | [https://github.com/hkproj/100-days-of-gpu](https://github.com/hkproj/100-days-of-gpu)
- **å¯¼å¸ˆ**: [https://github.com/hkproj](https://github.com/hkproj) | [https://github.com/hkproj/100-days-of-gpu](https://github.com/hkproj/100-days-of-gpu)

## Mandatory and Optional Tasks å¿…åšä»»åŠ¡å’Œé€‰åšä»»åŠ¡

| Day å¤© | Task Description ä»»åŠ¡æè¿° |
|--------|----------------------------|
| D15    | **Mandatory FA2-Forward**: Implement forward pass for FA2 (e.g., a custom neural network layer). å¼ºåˆ¶ FA2-Forward: å®ç° FA2 çš„å‰å‘ä¼ é€’ï¼ˆä¾‹å¦‚ï¼Œè‡ªå®šä¹‰ç¥ç»ç½‘ç»œå±‚ï¼‰ã€‚ |
| D20    | **Mandatory FA2-Backwards**: Implement backward pass for FA2 (e.g., gradient computation). å¼ºåˆ¶ FA2-åå‘ä¼ é€’: å®ç° FA2 çš„åå‘ä¼ é€’ï¼ˆä¾‹å¦‚ï¼Œæ¢¯åº¦è®¡ç®—ï¼‰ã€‚ |
| D20    | **Optional Fused Chunked CE Loss + Backwards**: Fused implementation of chunked cross-entropy loss with backward pass. Can use Liger Kernel as a reference implementation. å¯é€‰çš„èåˆåˆ†å—äº¤å‰ç†µæŸå¤± + åå‘ä¼ é€’: èåˆåˆ†å—äº¤å‰ç†µæŸå¤±çš„åå‘ä¼ é€’å®ç°ã€‚å¯ä»¥å‚è€ƒ Liger Kernel ä½œä¸ºå‚è€ƒå®ç°ã€‚ |

# Project Progress by Day

| Day   | Files & Summaries |
|-------|-------------------|
| day1  | **RMSnorm_vec.cu**: Implementation of RMS normalization using CUDA, including warp and block-level reduction for efficient computation. |
| day2  | **FP16_GELU.cu**: Implemented a vectorized FP16 GELU kernel and benchmarked it against PyTorch's implementation. |
| day3  | **gemv.cu**: Implemented a vectorized FP32 GEMV kernel and benchmarked it against PyTorch's implementation. **Issue**: Bandwidth measurement problem remains unresolved. |
| day4  | **per_tensor_quantize.cu**: Implemented per-tensor symmetric and asymmetric INT8 quantization. Results were verified on a CPU. |
| day5  | **fused_bias_mask_scale_and_add_fp16.cu**: Implemented a vectorized FP16 fused bias, mask, scale, and add kernel. |
| day6  | **optimized_kernel.cu**: |
|       | 1. v1: Added global memory coalesced access and shared memory caching |
|       | 2. v2: Implemented shared memory and sliding window technique |
|       | 3. v3: Developed strided shared memory access to improve compute intensity |
|       | 4. v4: Created vectorized reads based on v3 |
| day7  | **sgemm_kernel_fp32.cu**: |
|       | 1. v5: Implemented vectorized reads, but column-wise access of matrix B caused bank conflicts |
|       | 2. v6: Transformed inner product to outer product in shared memory, implemented secondary caching using registers, but without float4 reads |
|       | 3. v7: Based on v6, implemented float4 reads in N-direction of the block using register indexing |
|       | 4. v8: Added transpose storage of matrix A to shared memory via register buffering |
|       | 5. v9: Introduced double buffering technique using two shared memory buffers to hide memory loading latency |
| day8  | **attention_mask.cu**: |
|       | 1. å®ç°äº†åŸºæœ¬çš„attention mask CUDA kernelï¼Œæ”¯æŒä¸åŒåºåˆ—é•¿åº¦çš„maskè®¡ç®— |
|       | 2. è€ƒè™‘äº†repeat kvï¼ˆkey-valueé‡å¤ï¼‰çš„æƒ…å†µï¼Œä½†å°šæœªè¿›è¡Œå……åˆ†æµ‹è¯• |
| day9  | **cublas_gemm.cu**: |
|       | 1. å®ç°äº†ä½¿ç”¨cublasè¿›è¡ŒçŸ©é˜µä¹˜æ³•çš„åŸºæœ¬ç¤ºä¾‹ |
|       | 2. ç‰¹åˆ«æ³¨æ„ï¼šcublasé»˜è®¤ä½¿ç”¨åˆ—ä¸»åºå­˜å‚¨ï¼ˆcolumn-majorï¼‰ï¼Œä¸PyTorchçš„è¡Œä¸»åºï¼ˆrow-majorï¼‰ä¸åŒ |
|       | 3. è§£å†³æ–¹æ³•ï¼šåœ¨ä½¿ç”¨cublasæ—¶ï¼Œå¯ä»¥é€šè¿‡è½¬ç½®è¾“å…¥çŸ©é˜µæ¥æ¨¡æ‹Ÿè¡Œä¸»åºè¡Œä¸º |
|       | 4. æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨cublasLt APIè¿›è¡Œæ›´çµæ´»çš„çŸ©é˜µä¹˜æ³•é…ç½® |


